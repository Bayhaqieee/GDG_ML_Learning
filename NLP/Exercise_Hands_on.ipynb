{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Latihan - Pipeline Analisis Artikel Berita\n",
        "Tujuannya adalah membuat alat otomatis yang dapat memberikan ringkasan tingkat tinggi (topik) dan juga informasi detail (entitas) dari teks berita apa pun. Ini adalah tugas yang sangat umum di dunia industri, digunakan untuk menganalisis media, laporan keuangan, dan banyak lagi.\n",
        "\n",
        "Kita akan menggunakan dataset **AG News**, yang berisi ribuan artikel berita yang diklasifikasikan ke dalam 4 kategori: **World, Sports, Business, dan Sci/Tech.**"
      ],
      "metadata": {
        "id": "3dYq5BBBjq5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagian 1: Klasifikasi Topik Artikel"
      ],
      "metadata": {
        "id": "yyuEN9qSkLiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Memuat Data & Library"
      ],
      "metadata": {
        "id": "ww1zeavcm25T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from collections import defaultdict\n",
        "\n",
        "# Library untuk Klasifikasi\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Muat dataset (versi training)\n",
        "# Dataset ini memiliki kolom: Class Index, Title, Description\n",
        "# Kita akan menggabungkan Title dan Description\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv',\n",
        "                 names=['label', 'title', 'description'])\n",
        "\n",
        "# Mapping label dari angka ke teks\n",
        "label_map = {1: 'World', 2: 'Sports', 3: 'Business', 4: 'Sci/Tech'}\n",
        "df['topic'] = df['label'].map(label_map)\n",
        "\n",
        "# Gabungkan title dan description menjadi satu kolom teks\n",
        "df['text'] = ...\n",
        "\n",
        "# Pilih kolom yang relevan saja dan ambil sampel agar proses lebih cepat\n",
        "df = df[['text', 'topic']].sample(10000, random_state=42) # Ambil 10,000 sampel acak\n",
        "\n",
        "print(\"Dataset berhasil dimuat dan diproses.\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kkpG3C5JkT-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Vektorisasi dan Pembuatan Model"
      ],
      "metadata": {
        "id": "jmwr4H0Zm_f2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tentukan Fitur (X) dan Target (y)\n",
        "X = df['text']\n",
        "y = df['topic']\n",
        "\n",
        "# Bagi data menjadi training dan testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Inisialisasi dan fit TF-IDF Vectorizer\n",
        "tfidf = ...\n",
        "X_train_vec = ...\n",
        "X_test_vec = ...\n",
        "\n",
        "# Latih model Logistic Regression\n",
        "classifier = LogisticRegression(max_iter=1000)\n",
        "print(\"Melatih model klasifikasi...\")\n",
        "classifier.fit(X_train_vec, y_train)\n",
        "print(\"Model selesai dilatih!\")\n",
        "\n",
        "# Evaluasi model\n",
        "y_pred = classifier.predict(X_test_vec)\n",
        "print(\"\\nLaporan Klasifikasi:\\n\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NmGl7cKhm8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagian 2: Ekstraksi Entitas dengan spaCy"
      ],
      "metadata": {
        "id": "ERPhAl-JnHhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Muat model spaCy\n",
        "nlp = ...\n",
        "\n",
        "def extract_entities(text):\n",
        "    \"\"\"\n",
        "    Fungsi ini mengambil teks dan mengembalikan kamus entitas yang ditemukan.\n",
        "    \"\"\"\n",
        "    doc = ...\n",
        "    entities = defaultdict(list)\n",
        "    for ent in doc.ents:\n",
        "        entities[ent.label_].append(ent.text)\n",
        "\n",
        "    # Hapus duplikat\n",
        "    for key in entities:\n",
        "        entities[key] = list(set(entities[key]))\n",
        "\n",
        "    return dict(entities)\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"Elon Musk, CEO of SpaceX, announced a new mission to Mars from their headquarters in California.\"\n",
        "print(\"Contoh Ekstraksi Entitas:\")\n",
        "print(extract_entities(sample_text))"
      ],
      "metadata": {
        "id": "RWXaVZ9enFXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bagian 3: Menggabungkan Semuanya"
      ],
      "metadata": {
        "id": "aBTWZnvVnTek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_article(article_text):\n",
        "    \"\"\"\n",
        "    Pipeline lengkap:\n",
        "    1. Mengklasifikasikan topik artikel.\n",
        "    2. Mengekstrak entitas dari artikel.\n",
        "    3. Mencetak hasilnya dengan rapi.\n",
        "    \"\"\"\n",
        "    print(\"--- Menganalisis Artikel ---\")\n",
        "\n",
        "    # 1. Prediksi Topik\n",
        "    text_vec = ...\n",
        "    predicted_topic = ...\n",
        "\n",
        "    # 2. Ekstrak Entitas\n",
        "    entities_found = ...\n",
        "\n",
        "    # 3. Tampilkan Hasil\n",
        "    print(f\"\\nPrediksi Topik: **{predicted_topic}**\\n\")\n",
        "    print(\"--- Entitas yang Ditemukan ---\")\n",
        "    if not entities_found:\n",
        "        print(\"Tidak ada entitas yang ditemukan.\")\n",
        "    else:\n",
        "        for label, items in entities_found.items():\n",
        "            print(f\"- **{label}**: {', '.join(items)}\")\n",
        "\n",
        "    print(\"\\n--- Analisis Selesai ---\\n\")"
      ],
      "metadata": {
        "id": "GjFlP6iYnQiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uji Coba Pipeline Lengkap"
      ],
      "metadata": {
        "id": "Ah6N3WYJnZw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh 1: Artikel Bisnis/Teknologi\n",
        "article_1 = \"\"\"\n",
        "Microsoft Corp on Tuesday announced its next-generation Surface laptops,\n",
        "including a new model with a custom artificial intelligence chip, as it amps up its rivalry\n",
        "with Apple Inc ahead of the holiday shopping season in the United States.\n",
        "Satya Nadella presented the new features in a conference in New York.\n",
        "\"\"\"\n",
        "analyze_article(article_1)\n",
        "\n",
        "# Contoh 2: Artikel Olahraga\n",
        "article_2 = \"\"\"\n",
        "Real Madrid secured a dramatic late victory against Manchester City in the Champions League final\n",
        "held in Istanbul. A stunning goal from Vinicius Junior in the 88th minute sealed the win for\n",
        "the Spanish giants, leaving manager Pep Guardiola disappointed.\n",
        "\"\"\"\n",
        "analyze_article(article_2)"
      ],
      "metadata": {
        "id": "jAjI-wlbnWS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HrtgzrZ7neW6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}