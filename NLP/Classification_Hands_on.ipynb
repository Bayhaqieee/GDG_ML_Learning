{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Klasifikasi Teks - Analisis Sentimen Ulasan Film\n",
        "Analisis sentimen adalah proses memahami opini atau emosi di balik sebuah teks. Ini adalah tugas klasifikasi teks yang paling umum, digunakan oleh perusahaan untuk melacak sentimen pelanggan di media sosial, ulasan produk, dan banyak lagi.\n",
        "\n",
        "Kita akan menggunakan dataset **IMDb Movie Reviews** yang terkenal, yang berisi 50.000 ulasan film yang sudah diberi label positif atau negatif.\n"
      ],
      "metadata": {
        "id": "0ZvjFLMU2TEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Instalasi dan Import Library\n",
        "Pertama, kita akan menginstal dan mengimpor semua library yang dibutuhkan. Kita akan menggunakan scikit-learn untuk pemrosesan teks dan pembuatan model."
      ],
      "metadata": {
        "id": "XA_aWwRo2uCU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1we4gBvP2HBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Memuat dan Eksplorasi Data\n",
        "Kita akan memuat dataset dari file CSV dan melihat isinya untuk memahami strukturnya."
      ],
      "metadata": {
        "id": "kYlJUwL03_y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "CtCffu9r8WDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
        "\n",
        "# Tampilkan 5 baris pertama\n",
        "print(\"Data Head:\")\n",
        "print(df.head())\n",
        "\n",
        "# Lihat distribusi sentimen\n",
        "print(\"\\nDistribusi Sentimen:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Visualisasikan distribusi\n",
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.title('Distribusi Sentimen Ulasan Film')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LgqmXvdn6spJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Preprocessing Teks\n",
        "Teks mentah tidak bisa langsung dimasukkan ke model. Kita perlu membersihkannya terlebih dahulu. Proses ini meliputi:\n",
        "*   Mengubah teks menjadi huruf kecil (lowercase).\n",
        "*   Menghapus tag HTML.\n",
        "*   Menghapus karakter non-alfabet (seperti tanda baca dan angka).\n",
        "*   Menghapus spasi berlebih."
      ],
      "metadata": {
        "id": "N4njCm8r6lFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Mengubah ke huruf kecil\n",
        "    text = text.lower()\n",
        "    # Menghapus tag HTML\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Menghapus karakter non-alfabet\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Menghapus spasi berlebih\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Terapkan fungsi preprocessing ke kolom 'review'\n",
        "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "print(\"Contoh Teks Asli:\")\n",
        "print(df['review'][0])\n",
        "print(\"\\nContoh Teks Setelah Dibersihkan:\")\n",
        "print(df['cleaned_review'][0])"
      ],
      "metadata": {
        "id": "MaU7YS34-L9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Vektorisasi: Mengubah Teks menjadi Angka\n",
        "Model Machine Learning hanya mengerti angka. **TF-IDF (Term Frequency-Inverse Document Frequency)** adalah teknik populer untuk mengubah teks menjadi vektor numerik. TF-IDF akan mengukur seberapa penting sebuah kata dalam sebuah dokumen relatif terhadap keseluruhan koleksi dokumen (corpus)."
      ],
      "metadata": {
        "id": "1DPBuJnW6rVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi TF-IDF Vectorizer\n",
        "# max_features=5000 artinya kita hanya akan mengambil 5000 kata paling penting\n",
        "# stop_words='english' akan menghapus kata-kata umum dalam bahasa Inggris (seperti 'the', 'a', 'is')\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "# Ubah teks menjadi fitur TF-IDF\n",
        "X = tfidf.fit_transform(df['cleaned_review']).toarray()\n",
        "\n",
        "# Ubah label sentimen menjadi angka (positive=1, negative=0)\n",
        "y = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
        "\n",
        "print(\"Dimensi matriks fitur (X):\", X.shape)\n",
        "print(\"Dimensi vektor target (y):\", y.shape)"
      ],
      "metadata": {
        "id": "t1hglIeb_tKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Membagi Data: Training dan Testing\n",
        "Kita perlu membagi data kita menjadi dua bagian:\n",
        "\n",
        "*   Data Training (80%): Untuk melatih model.\n",
        "*   Data Testing (20%): Untuk menguji seberapa baik performa model pada data yang belum pernah dilihat sebelumnya.\n",
        "\n"
      ],
      "metadata": {
        "id": "1kKBYUbP6rlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(\"Ukuran data training:\", X_train.shape)\n",
        "print(\"Ukuran data testing:\", X_test.shape)"
      ],
      "metadata": {
        "id": "b9f1Dp-fAMCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Membangun dan Melatih Model\n",
        "Kita akan menggunakan model klasifikasi sederhana namun kuat, yaitu **Logistic Regression**."
      ],
      "metadata": {
        "id": "eMnvRb-F6rw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inisialisasi model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Latih model dengan data training\n",
        "print(\"Melatih model...\")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Model selesai dilatih!\")"
      ],
      "metadata": {
        "id": "lunU-DAgAcis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluasi Model\n",
        "Sekarang, kita evaluasi performa model menggunakan data testing. Kita akan melihat **akurasi** dan **confusion matrix.**"
      ],
      "metadata": {
        "id": "mrYiTuRD6r7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakukan prediksi pada data test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Hitung akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Akurasi Model: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Tampilkan classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "# Tampilkan confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pz8MelP3A3FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Tes dengan Ulasan Baru"
      ],
      "metadata": {
        "id": "1C-uDpiN6sEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(review_text):\n",
        "    # 1. Bersihkan teks\n",
        "    cleaned_text = preprocess_text(review_text)\n",
        "    # 2. Ubah teks menjadi vektor TF-IDF\n",
        "    vectorized_text = tfidf.transform([cleaned_text]).toarray()\n",
        "    # 3. Lakukan prediksi\n",
        "    prediction = model.predict(vectorized_text)\n",
        "    # 4. Tampilkan hasil\n",
        "    if prediction[0] == 1:\n",
        "        print(\"Sentimen: Positif\")\n",
        "    else:\n",
        "        print(\"Sentimen: Negatif\")\n",
        "\n",
        "# Uji coba\n",
        "predict_sentiment(\"The acting was superb and the plot was thrilling.\")\n",
        "predict_sentiment(\"A complete waste of time.\")"
      ],
      "metadata": {
        "id": "VNE4Tw9H3uGL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}